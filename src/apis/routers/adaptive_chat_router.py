from fastapi import APIRouter, status, BackgroundTasks
from fastapi.responses import JSONResponse, StreamingResponse
from src.utils.logger import logger
import json
import uuid
from langchain_core.messages.ai import AIMessageChunk

from src.apis.interfaces.adaptive_chat_interface import (
    AdaptiveChatBody,
    AdaptiveChatResponse,
)
from src.agents.adaptive_chatbot.flow import adaptive_chatbot_agent
from src.agents.adaptive_chatbot.data import Message


router = APIRouter(prefix="/adaptive-chat", tags=["Adaptive Chatbot"])


async def message_generator(input_graph: dict, background: BackgroundTasks):
    """
    Stream the messages generated by the adaptive chatbot.
    
    Args:
        input_graph: Input state for the agent
        background: Background tasks
        
    Yields:
        JSON formatted messages for streaming response
    """
    try:
        last_output_state = None
        temp = ""

        try:
            async for event in adaptive_chatbot_agent.astream(
                input=input_graph,
                stream_mode=["messages", "values"],
            ):
                try:
                    event_type, event_message = event
                    if event_type == "messages":
                        message, _ = event_message
                        if isinstance(message, AIMessageChunk):
                            temp += message.content
                            yield json.dumps(
                                {
                                    "type": "message",
                                    "content": temp,
                                },
                                ensure_ascii=False,
                            ) + "\n\n"
                    if event_type == "values":
                        last_output_state = event_message
                except Exception as e:
                    logger.error(f"Error processing stream event: {str(e)}")
                    yield json.dumps(
                        {
                            "type": "error",
                            "content": "Error processing response: " + str(e),
                        },
                        ensure_ascii=False,
                    ) + "\n\n"
                    return

            if last_output_state is None:
                raise ValueError("No output state received from workflow")

            if "bot_message" not in last_output_state:
                raise ValueError("No bot message in output")

            try:
                # Check if we have probing questions
                if last_output_state.get("probing_questions") and len(last_output_state.get("probing_questions", [])) > 0:
                    final_response = json.dumps(
                        {
                            "type": "final",
                            "content": {
                                "probing_questions": last_output_state["probing_questions"],
                                "session_id": last_output_state["session_id"],
                            },
                        },
                        ensure_ascii=False,
                    )
                else:
                    final_response = json.dumps(
                        {
                            "type": "final",
                            "content": {
                                "bot_message": last_output_state["bot_message"],
                                "updated_system_prompt": last_output_state.get("updated_system_prompt"),
                                "session_id": last_output_state["session_id"],
                                "user_profile_updates": last_output_state.get("user_profile"),
                            },
                        },
                        ensure_ascii=False,
                    )
                yield final_response + "\n\n"
            except Exception as e:
                logger.error(f"Error processing final response: {str(e)}")
                yield json.dumps(
                    {
                        "type": "error",
                        "content": "Error processing the final response: " + str(e),
                    },
                    ensure_ascii=False,
                ) + "\n\n"
                return

        except Exception as e:
            logger.error(f"Error in workflow stream: {str(e)}")
            yield json.dumps(
                {"type": "error", "content": "Error processing stream: " + str(e)},
                ensure_ascii=False,
            ) + "\n\n"
            return

    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        yield json.dumps(
            {"type": "error", "content": "An unexpected error occurred: " + str(e)},
            ensure_ascii=False,
        ) + "\n\n"
        return


@router.post("/chat/stream")
async def adaptive_chat_stream(body: AdaptiveChatBody, background: BackgroundTasks):
    """
    Stream the response from the adaptive chatbot.
    
    Args:
        body: Chat request body
        background: Background tasks
        
    Returns:
        Streaming response
    """
    try:
        session_id = body.session_id if body.session_id else str(uuid.uuid4())
        
        # Convert ChatMessage objects to dictionaries if needed
        messages_history = []
        if body.history:
            for msg in body.history:
                messages_history.append({
                    "content": msg.content,
                    "type": msg.type
                })
        
        return StreamingResponse(
            message_generator(
                input_graph={
                    "user_message": body.query,
                    "session_id": session_id,
                    "messages_history": messages_history,
                    "current_system_prompt": body.current_system_prompt,
                    "user_profile": body.user_profile if body.user_profile else {},
                    "messages": [(
                        "human", 
                        body.query
                    )],
                },
                background=background,
            ),
            media_type="text/event-stream",
        )
    except Exception as e:
        logger.error(f"Error in streaming endpoint: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"error": f"Streaming error: {str(e)}"},
        )


@router.post("/chat", response_model=AdaptiveChatResponse)
async def adaptive_chat(body: AdaptiveChatBody):
    """
    Get a non-streaming response from the adaptive chatbot.
    
    Args:
        body: Chat request body
        
    Returns:
        Chat response
    """
    logger.info(f"Adaptive chat request: {body}")
    
    session_id = body.session_id if body.session_id else str(uuid.uuid4())
    
    # Convert ChatMessage objects to dictionaries if needed
    messages_history = []
    if body.history:
        for msg in body.history:
            messages_history.append({
                "content": msg.content,
                "type": msg.type
            })
    
    response = await adaptive_chatbot_agent.ainvoke(
        {
            "user_message": body.query,
            "session_id": session_id,
            "messages_history": messages_history,
            "current_system_prompt": body.current_system_prompt,
            "user_profile": body.user_profile if body.user_profile else {},
            "messages": [(
                "human", 
                body.query
            )],
        }
    )
    
    # Check if we have probing questions
    if response.get("probing_questions") and len(response.get("probing_questions", [])) > 0:
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "bot_message": "",  # Empty bot message
                "updated_system_prompt": response.get("updated_system_prompt"),
                "session_id": response["session_id"],
                "probing_questions": response["probing_questions"],
                "user_profile_updates": response.get("user_profile"),
            },
        )
    else:
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "bot_message": response["bot_message"],
                "updated_system_prompt": response.get("updated_system_prompt"),
                "session_id": response["session_id"],
                "probing_questions": None,
                "user_profile_updates": response.get("user_profile"),
            },
        ) 